{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f4cf61-43f4-4290-b87e-903b74fedcfe",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG\"\n",
    "icon: \"search\"\n",
    "---\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a method for enhancing the responses of language models by injecting external knowledge at generation time. Instead of relying solely on what the model \"knows\" (from training), RAG enables the model to query external sources‚Äîlike search engines, databases, APIs, or custom document stores‚Äîto access the most relevant and up-to-date information.\n",
    "\n",
    "### RAG Architectures\n",
    "\n",
    "RAG can be implemented in multiple ways, depending on your system's needs:\n",
    "\n",
    "- **2-Step RAG**: Retrieval always happens before generation. Simple and predictable.\n",
    "- **Agentic RAG**: An LLM-powered agent decides *when* and *how* to retrieve during reasoning.\n",
    "\n",
    "![rag architectures](./rag_systems.png)\n",
    "\n",
    "\n",
    "| Architecture | Control   | Flexibility | Example Use Case         |\n",
    "| ------------ | --------- | ----------- | ------------------------ |\n",
    "| 2-Step RAG   | ‚úÖ High    | ‚ùå Low       | FAQs, documentation bots |\n",
    "| Hybrid       | ‚öñÔ∏è Medium | ‚öñÔ∏è Medium   | Technical Q\\&A           |\n",
    "| Agentic RAG  | ‚ùå Low     | ‚úÖ High      | Research assistants      |\n",
    "\n",
    "\n",
    "\n",
    "## Building a knowledge base\n",
    "\n",
    "Section contains cross-links to documentation about vectorstores and custom retrievers.\n",
    "\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è Implementation\n",
    "\n",
    "We‚Äôll walk through three progressively more dynamic implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede709a-e5c4-4aa5-a083-9a052f2884e7",
   "metadata": {},
   "source": [
    "## 1. Agentic RAG\n",
    "\n",
    "**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.\n",
    "\n",
    "<Tip>\n",
    "The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge ‚Äî such as documentation loaders, web APIs, or database queries. This tool-based architecture makes Agentic RAG modular, flexible, and ideal for evolving knowledge environments.\n",
    "</Tip>\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User Input / Question] --> B[\"Agent (LLM)\"]\n",
    "    B --> C{Need external info?}\n",
    "    C -- Yes --> D[\"Search using tool(s)\"]\n",
    "    D --> H{Enough to answer?}\n",
    "    H -- No --> B\n",
    "    H -- Yes --> I[Generate final answer]\n",
    "    C -- No --> I\n",
    "    I --> J[Return to user]\n",
    "\n",
    "    %% Dark-mode friendly styling\n",
    "    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n",
    "    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n",
    "    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n",
    "\n",
    "    class A,J startend\n",
    "    class B,D,I process\n",
    "    class C,H decision\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model('claude-sonnet-4-0', max_tokens=32_000)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    # Include tools that include retrieval tools\n",
    "    tools=tools, # [!code highlight] \n",
    "    # Customize the prompt with instructions on how to retrieve\n",
    "    # the data.\n",
    "    prompt=system_prompt,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "### üß™ Example: Agentic RAG with LangGraph Documentation\n",
    "\n",
    "This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading `llms.txt`, which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user‚Äôs question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2383a10c-4127-42b0-83a2-ce81365382cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from markdownify import markdownify\n",
    "import requests\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "ALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"]\n",
    "LLMS_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt'\n",
    "\n",
    "@tool\n",
    "def fetch_documentation(url: str) -> str:\n",
    "    \"\"\"Fetch and convert documentation from a URL\"\"\"\n",
    "    if not any(url.startswith(domain) for domain in ALLOWED_DOMAINS):\n",
    "        return f\"Error: URL not allowed. Must start with one of: {', '.join(ALLOWED_DOMAINS)}\"\n",
    "    response = requests.get(url, timeout=10.0)\n",
    "    response.raise_for_status()\n",
    "    return markdownify(response.text)\n",
    "\n",
    "# We will fetch the content of llms.txt, so this can be done ahead of time without requiring an LLM request.\n",
    "llms_txt_content = requests.get(LLMS_TXT).text\n",
    "\n",
    "# System prompt for the agent\n",
    "system_prompt = f\"\"\"\n",
    "You are an expert Python developer and technical assistant. \n",
    "Your primary role is to help users with questions about LangGraph and related tools.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. If a user asks a question you're unsure about ‚Äî or one that likely involves API usage, \n",
    "   behavior, or configuration ‚Äî you MUST use the `fetch_documentation` tool to consult the relevant docs.\n",
    "2. When citing documentation, summarize clearly and include relevant context from the content.\n",
    "3. Do not use any URLs outside of the allowed domain.\n",
    "4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\n",
    "\n",
    "You can access official documentation from the following approved sources:\n",
    "\n",
    "{llms_txt_content}\n",
    "\n",
    "You MUST consult the documentation to get up to date documentation \n",
    "before answering a user's question about LangGraph.\n",
    "\n",
    "Your answers should be clear, concise, and technically accurate.\n",
    "\"\"\"\n",
    "\n",
    "tools = [fetch_documentation]\n",
    "\n",
    "model = init_chat_model('claude-sonnet-4-0', max_tokens=32_000)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    prompt=system_prompt,\n",
    "    name=\"Agentic RAG\",\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [{\n",
    "        'role': 'user',\n",
    "        'content': (\n",
    "            \"Write a short example of a langgraph agent using the \"\n",
    "            \"prebuilt create react agent. the agent should be able \"\n",
    "            \"to loook up stock pricing information.\"\n",
    "        )\n",
    "    }]\n",
    "})\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec52e1c-3b5f-48ae-b1de-6f49f6786a66",
   "metadata": {},
   "source": [
    "# 2. Retrieval -> Generation workflow\n",
    "\n",
    "- **2-Step RAG**: Retrieval always happens before generation.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User Question] --> B[\"Retrieve Relevant Documents\"]\n",
    "    B --> C[\"Generate Answer\"]\n",
    "    C --> D[Return Answer to User]\n",
    "\n",
    "    %% Styling\n",
    "    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n",
    "    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n",
    "\n",
    "    class A,D startend\n",
    "    class B,C process\n",
    "```\n",
    "\n",
    "### üß™ Example: Working with LangGraph GitHub issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c8065af8-c5a1-4e08-baef-f0a758096ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the recent GitHub issues, here are the main themes:\n",
      "\n",
      "## **Documentation & Developer Experience**\n",
      "- Multiple documentation improvement requests, including:\n",
      "  - Command.goto behavior with static edges ([#5829](https://github.com/langchain-ai/langgraph/issues/5829))\n",
      "  - RemainingSteps managed value clarity ([#5775](https://github.com/langchain-ai/langgraph/issues/5775))\n",
      "  - Message history with memory ([#5773](https://github.com/langchain-ai/langgraph/issues/5773))\n",
      "  - Checkpointer documentation for subgraphs ([#5734](https://github.com/langchain-ai/langgraph/issues/5734))\n",
      "  - Self-hosted lite setup issues ([#5754](https://github.com/langchain-ai/langgraph/issues/5754))\n",
      "\n",
      "## **CLI & Development Tools Issues**\n",
      "- Problems with `langgraph dev` command:\n",
      "  - Ignoring checkpointer configuration ([#5790](https://github.com/langchain-ai/langgraph/issues/5790))\n",
      "  - Windows path issues in build/dockerfile commands ([#5815](https://github.com/langchain-ai/langgraph/issues/5815))\n",
      "  - HTTP client resource errors ([#5766](https://github.com/langchain-ai/langgraph/issues/5766))\n",
      "\n",
      "## **Serialization & Data Handling Problems**\n",
      "- JSON serialization issues affecting multiple components:\n",
      "  - PostgresSaver serialization ([#5769](https://github.com/langchain-ai/langgraph/issues/5769))\n",
      "  - Pydantic model caching ([#5733](https://github.com/langchain-ai/langgraph/issues/5733))\n",
      "  - Send objects in output events ([#5725](https://github.com/langchain-ai/langgraph/issues/5725))\n",
      "  - Tool argument parsing ([#5704](https://github.com/langchain-ai/langgraph/issues/5704))\n",
      "\n",
      "## **Tool Integration & LLM Interaction Issues**\n",
      "- Various tool-related problems:\n",
      "  - OpenRouter tool type errors ([#5822](https://github.com/langchain-ai/langgraph/issues/5822))\n",
      "  - Parameterless tool invocation errors ([#5722](https://github.com/langchain-ai/langgraph/issues/5722))\n",
      "  - Tool usage result handling ([#5760](https://github.com/langchain-ai/langgraph/issues/5760))\n",
      "\n",
      "## **Runtime & Streaming Functionality**\n",
      "- Runtime context and streaming issues:\n",
      "  - Null runtime context from stream endpoint ([#5804](https://github.com/langchain-ai/langgraph/issues/5804))\n",
      "  - Missing error events in debug streaming ([#5764](https://github.com/langchain-ai/langgraph/issues/5764))\n",
      "  - Runtime support improvements needed ([#5776](https://github.com/langchain-ai/langgraph/issues/5776))\n",
      "\n",
      "## **Graph Structure & Command Pattern**\n",
      "- Issues with graph behavior and commands:\n",
      "  - Virtual edge creation problems with Commands ([#5772](https://github.com/langchain-ai/langgraph/issues/5772))\n",
      "  - RemoveMessage not working across subgraphs ([#5755](https://github.com/langchain-ai/langgraph/issues/5755))\n",
      "  - Caching not considering function code changes ([#5820](https://github.com/langchain-ai/langgraph/issues/5820))\n",
      "\n",
      "## **Code Quality & Maintenance**\n",
      "- Internal refactoring and improvement tasks:\n",
      "  - React agent refactoring ([#5710](https://github.com/langchain-ai/langgraph/issues/5710), [#5692](https://github.com/langchain-ai/langgraph/issues/5692))\n",
      "  - Type annotation updates ([#5739](https://github.com/langchain-ai/langgraph/issues/5739))\n",
      "  - Import test suite addition ([#5810](https://github.com/langchain-ai/langgraph/issues/5810))\n",
      "\n",
      "The issues suggest LangGraph is actively being developed with focus on improving reliability, developer experience, and fixing integration problems with various LLM providers and tools.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import TypedDict, NotRequired\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    retrieved_content: NotRequired[str]\n",
    "    answer: NotRequired[str]\n",
    "\n",
    "llm = init_chat_model('claude-sonnet-4-0', max_tokens=32000)\n",
    "\n",
    "\n",
    "def retrieval_step(state: GraphState) -> GraphState:\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"User-Agent\": \"langgraph-rag-example\",\n",
    "    }\n",
    "\n",
    "    url = \"https://api.github.com/repos/langchain-ai/langgraph/issues\"\n",
    "    params = {\n",
    "        \"state\": \"open\",\n",
    "        \"per_page\": 50,\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    items = response.json()\n",
    "    base_url = \"https://github.com/langchain-ai/langgraph/issues/\"\n",
    "    # Filter out PRs (issues with \"pull_request\" key are actually PRs)\n",
    "    issues = [f\"- {issue['title']} {base_url}{issue['number']}\" for issue in items if \"pull_request\" not in issue]\n",
    "    retrieved = \"\\n\".join(issues) if issues else \"No issues found.\"\n",
    "    \n",
    "    return {\n",
    "        \"retrieved_content\": retrieved\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_response(state: GraphState) -> GraphState:\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant. Use the following GitHub issue data to answer the user's question. \"\n",
    "                \"When relevant also include urls to the issues in the response.\\n\\n---\\n\\n\"\n",
    "                f\"Retrieved GitHub Issues:\\n{state['retrieved_content']}\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": state[\"question\"]\n",
    "        }\n",
    "    ]\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"retrieved_content\": state[\"retrieved_content\"],\n",
    "        \"answer\": response.content\n",
    "    }\n",
    "\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"retrieval\", retrieval_step)\n",
    "builder.add_node(\"generation\", generate_response)\n",
    "builder.set_entry_point(\"retrieval\")\n",
    "builder.add_edge(\"retrieval\", \"generation\")\n",
    "builder.add_edge(\"generation\", END)\n",
    "\n",
    "graph = builder.compile(name=\"2-step rag\")\n",
    "\n",
    "response = graph.invoke({\n",
    "    \"question\": \"What are the themes in the recent issues?\",\n",
    "})\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2e6f7-3d5e-4fcc-b807-64262de10fcf",
   "metadata": {},
   "source": [
    "## 3. Hybrid architectures\n",
    "\n",
    "There are many possible variations on RAG architectures.\n",
    "\n",
    "\n",
    "1. The retrieval step can involve an LLM to either interpret the question, to re-write the question or write multiple versions of it.\n",
    "2. Reflection steps after retrieval: to decide whether retrieved results make sense and if not re-execute retrieval.\n",
    "3. Reflection steps after generation: to decide whether the the generated answer is good and if not, to try re-execute retrieval or generation.\n",
    "4. Variations could allow for up to a certain number of loop iterations that invclude retrieval and post generation etc.\n",
    "\n",
    "Here's an example of \n",
    "\n",
    "Examples\n",
    "\n",
    "* [Agentic RAG with Self correction](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
